# Simultaneous Machine Translation with Adapters (SiMT_NLP)

A PyTorch implementation of fixed and adaptive simultaneous machine translation strategies using adapter modules, based on the paper ["Fixed and Adaptive Simultaneous Machine Translation Strategies Using Adapters"](https://arxiv.org/pdf/2407.13469).

## Features
- ğŸš€ Transformer model with lightweight adapter modules
- â±ï¸ Two inference strategies:
  - Fixed wait-k policy
  - Adaptive threshold-based strategy
- ğŸ”„ Multi-path training with parameter sharing
- ğŸ“Š Latency-quality tradeoff management
- ğŸ’¾ Checkpoint averaging and management

## Installation
1. Clone the repository:
```bash
git clone https://github.com/Sandeep-4469/SiMT_NLP.git
cd SiMT_NLP

pip install -r requirements.txt

SiMT_NLP/
â”‚
â”œâ”€â”€ algo2.py          # Adapter module implementation for Transformer
â”œâ”€â”€ eval.py             # Transformer model with adapter integration
â”œâ”€â”€ train.py             # Utility functions (padding, tokenization, etc.)
â”œâ”€â”€ dataset.py           # Dataset loading and preprocessing
â”œâ”€â”€ train.py             # Training script with checkpoint management
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ run.sh               # Training automation script
â””â”€â”€ README.md            # Project documentation

python3 train.py